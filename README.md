# Model Extraction(Stealing) Attacks and Defenses on Machine Learning Models Literature

A curated list of membership inference attacks and defenses papers on machine learning models.

Papers are sorted by their released dates in descending order.

## Survey Papers
| Year | Title | Venue | Paper Link | Code Link |
|------|-------|-------|------------|-----------|
| 2024 | A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges | arXiv | [Link](http://arxiv.org/abs/2403.04468) | |
| 2024 | Graph neural networks: a survey on the links between privacy and security | Artificial Intelligence Review | [Link](https://doi.org/10.1007/s10462-023-10656-4) | |
| 2024 | Trustworthy Graph Neural Networks: Aspects, Methods and Trends | Proceedings of the IEEE | [Link](http://arxiv.org/abs/2205.07424) | |
| 2024 | Safety in Graph Machine Learning: Threats and Safeguards | arXiv | [Link](http://arxiv.org/abs/2405.11034) | |
| 2023 | A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications | arXiv | [Link](http://arxiv.org/abs/2308.16375) | |
| 2023 | A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability | arXiv | [Link](http://arxiv.org/abs/2204.08570) | |
| 2023 | A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly | ScienceDirect | [Link](https://www.sciencedirect.com/science/article/pii/S266729522400014X) | |
| 2023 | I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences | ACM Computing Surveys | [Link](https://doi.org/10.1145/3595292) | |
| 2022 | Membership Inference Attacks on Machine Learning: A Survey | ACM Computing Surveys | [Link](https://doi.org/10.1145/3523273) | |
| 2022 | Privacy and Robustness in Federated Learning: Attacks and Defenses | IEEE Transactions on Neural Networks and Learning Systems | [Link](https://ieeexplore.ieee.org/abstract/document/9945997?casa_token=x9pGEwKp2ocAAAAA:hln9lI37WYKjlaMyIM6J1CaBM_UzTUw3EeIARefaHEbzXXSEPyQt0tPFuREPlvtTBIiUpoVYQj0) | |
| 2022 | Towards Security Threats of Deep Learning Systems: A Survey | IEEE Transactions on Software Engineering | [Link](https://ieeexplore.ieee.org/document/9252914) | |


## Model Extraction Attack


This repository contains a curated list of research papers on model extraction attacks in machine learning, organized by year of publication.

### Table of Contents

- [2024](#2024)
- [2023](#2023)
- [2022](#2022)
- [2021](#2021)
- [2020](#2020)
- [2019](#2019)
- [2018](#2018)
- [2017](#2017)
- [2016](#2016)
- [2014](#2014)

### 2024

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2024 | Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2405.05990) | |
| 2024 | LLM-FIN: Large Language Models Fingerprinting Attack on Edge Devices | Large Language Models | ISQED | [Link](https://ieeexplore.ieee.org/abstract/document/10528736) | |
| 2024 | Model Extraction Attack against On-device Deep Learning with Power Side Channel | Deep Neural Networks | ISQED | [Link](https://ieeexplore.ieee.org/abstract/document/10528716) | |
| 2024 | Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2405.12295) | |
| 2024 | A realistic model extraction attack against graph neural networks | Graph Neural Networks | Knowledge-Based Systems | [Link](https://www.sciencedirect.com/science/article/pii/S0950705124007780) | |
| 2024 | Large Language Models for Link Stealing Attacks Against Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2406.16963) | |
| 2024 | Link Stealing Attacks Against Inductive Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2405.05784) | |
| 2024 | Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels | CNN | arXiv | [Link](http://arxiv.org/abs/2402.11953) | |
| 2024 | SwiftTheft: A Time-Efficient Model Extraction Attack Framework Against Cloud-Based Deep Neural Networks | Deep Neural Networks | Chinese Journal of Electronics | [Link](https://ieeexplore.ieee.org/abstract/document/10410588) | |
| 2024 | Model Extraction Attack Without Natural Images | Deep Neural Networks | ACNS Workshops | | |
| 2024 | DEMISTIFY: Identifying On-device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps | Mobile ML Models | ICSE | [Link](https://doi.org/10.1145/3597503.3623325) | |
| 2024 | Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble Based Sample Selection | Deep Neural Networks | WACV | [Link](https://openaccess.thecvf.com/content/WACV2024/html/Jindal_Army_of_Thieves_Enhancing_Black-Box_Model_Extraction_via_Ensemble_Based_WACV_2024_paper.html) | |
| 2024 | Trained to Leak: Hiding Trojan Side-Channels in Neural Network Weights | Neural Networks | HOST | [Link](https://ieeexplore.ieee.org/abstract/document/10545350) | |
| 2024 | A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning | Split Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_A_Stealthy_Wrongdoer_Feature-Oriented_Reconstruction_Attack_against_Split_Learning_CVPR_2024_paper.html) | |
| 2024 | Prompt Stealing Attacks Against Large Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2402.12959) | |
| 2024 | Stealing Part of a Production Language Model | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2403.06634) | |
| 2024 | Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing | Deep Neural Networks | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Fully_Exploiting_Every_Real_Sample_SuperPixel_Sample_Gradient_Model_Stealing_CVPR_2024_paper.html) | |
| 2024 | Teach LLMs to Phish: Stealing Private Information from Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2403.00871) | |
| 2024 | Data-Free Hard-Label Robustness Stealing Attack | Deep Neural Networks | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/28510) | |
| 2024 | Large Language Model Watermark Stealing With Mixed Integer Programming | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2405.19677) | |
| 2024 | PRSA: PRompt Stealing Attacks against Large Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2402.19200) | |
| 2024 | QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines | Quantum Neural Networks | arXiv | [Link](http://arxiv.org/abs/2403.10790) | |
| 2024 | Privacy Backdoors: Stealing Data with Corrupted Pretrained Models | Pretrained Models | arXiv | [Link](http://arxiv.org/abs/2404.00473) | |
| 2024 | AugSteal: Advancing Model Steal With Data Augmentation in Active Learning Frameworks | Active Learning Models | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10490222) | |
| 2024 | Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation | GANs | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/29966) | |
| 2024 | Stealing Image-to-Image Translation Models With a Single Query | Image-to-Image Translation Models | arXiv | [Link](http://arxiv.org/abs/2406.00828) | |
| 2024 | A two-stage model extraction attack on GANs with a small collected dataset | GANs | Computers & Security | [Link](https://www.sciencedirect.com/science/article/pii/S0167404823005448) | |
| 2024 | Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks | Deep Neural Networks | IEEE TCAD | [Link](https://ieeexplore.ieee.org/abstract/document/10500842) | |
| 2024 | COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2402.08679) | |
| 2024 | Prompt Stealing Attacks Against Text-to-Image Generation Models | Text-to-Image Models | arXiv | [Link](http://arxiv.org/abs/2302.09923) | |

### 2023

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2023 | Explanation leaks: Explanation-guided model extraction attacks | Explainable AI Models | Information Sciences | [Link](https://www.sciencedirect.com/science/article/pii/S002002552300316X) | |
| 2023 | Model Leeching: An Extraction Attack Targeting LLMs | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2309.10544) | |
| 2023 | Are Diffusion Models Vulnerable to Membership Inference Attacks? | Diffusion Models | ICML | [Link](https://proceedings.mlr.press/v202/duan23b.html) | |
| 2023 | Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2307.04401) | |
| 2023 | Model Extraction Attacks on Split Federated Learning | Federated Learning Models | arXiv | [Link](http://arxiv.org/abs/2303.08581) | |
| 2023 | Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders | Image Encoders | arXiv | [Link](http://arxiv.org/abs/2201.07513) | |
| 2023 | Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2312.10943) | |
| 2023 | Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack | Machine Learning Models | NeurIPS | [Link](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e5440ffceaf4831b5f98652b8a27ffde-Abstract-Conference.html) | |
| 2023 | D-DAE: Defense-Penetrating Model Extraction Attacks | Deep Neural Networks | IEEE S&P | [Link](https://ieeexplore.ieee.org/abstract/document/10179406) | |
| 2023 | Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models | Image Translation Models | arXiv | [Link](http://arxiv.org/abs/2104.12623) | |
| 2023 | AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models | White-Box Models | arXiv | [Link](http://arxiv.org/abs/2302.02162) | |
| 2023 | Efficient Model Extraction by Data Set Stealing, Balancing, and Filtering | Machine Learning Models | IEEE IoT Journal | [Link](https://ieeexplore.ieee.org/abstract/document/10214537) | |
| 2023 | DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer | Machine Learning Models | IEEE TDSC | [Link](https://ieeexplore.ieee.org/abstract/document/10007048) | |

### 2022

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2022 | Canary Extraction in Natural Language Understanding Models | NLU Models | arXiv | [Link](http://arxiv.org/abs/2203.13920) | |
| 2022 | Are Large Pre-Trained Language Models Leaking Your Personal Information? | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2205.12628) | |
| 2022 | Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers | Transformers | arXiv | [Link](http://arxiv.org/abs/2209.10505) | |
| 2022 | Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices | Deep Learning Models | ESORICS | | |
| 2022 | DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories | Deep Neural Networks | IEEE S&P | [Link](https://ieeexplore.ieee.org/abstract/document/9833743) | |
| 2022 | DualCF: Efficient Model Extraction Attack from Counterfactual Explanations | Machine Learning Models | FAccT | [Link](https://doi.org/10.1145/3531146.3533188) | |
| 2022 | GAME: Generative-Based Adaptive Model Extraction Attack | Machine Learning Models | ESORICS | | |
| 2022 | Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks | Deep Neural Networks | NeurIPS | [Link](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ed189de2611f200bd4c2ab30c576e99e-Abstract-Conference.html) | |
| 2022 | On the Difficulty of Defending Self-Supervised Learning against Model Extraction | Self-Supervised Learning Models | ICML | [Link](https://proceedings.mlr.press/v162/dziedzic22a.html) | |
| 2022 | Black-Box Dissector: Towards Erasing-Based Hard-Label Model Stealing Attack | Machine Learning Models | ECCV | | |
| 2022 | Imitated Detectors: Stealing Knowledge of Black-box Object Detectors | Object Detectors | ACM MM | [Link](https://doi.org/10.1145/3503161.3548416) | |
| 2022 | StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning | Self-Supervised Learning Models | ACM CCS | [Link](https://doi.org/10.1145/3548606.3560586) | |
| 2022 | Model Stealing Attacks Against Vision-Language Models | Vision-Language Models | OpenReview | [Link](https://openreview.net/forum?id=v-rx235RlfI) | |
| 2022 | ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles | Deep Neural Networks | IEEE TETCI | [Link](https://ieeexplore.ieee.org/abstract/document/9726514) | |
| 2022 | Data Stealing Attack on Medical Images: Is It Safe to Export Networks from Data Lakes? | Medical Image Models | MICCAI | | |
| 2022 | Towards Data-Free Model Stealing in a Hard Label Setting | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2022/html/Sanyal_Towards_Data-Free_Model_Stealing_in_a_Hard_Label_Setting_CVPR_2022_paper.html) | |
| 2022 | Careful What You Wish For: on the Extraction of Adversarially Trained Models | Adversarially Trained Models | PST | [Link](https://ieeexplore.ieee.org/abstract/document/9851981) | |
| 2022 | Enhance Model Stealing Attack via Label Refining | Machine Learning Models | ICSP | [Link](https://ieeexplore.ieee.org/document/9778562) | |
| 2022 | StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning | Self-Supervised Learning Models | arXiv | [Link](http://arxiv.org/abs/2201.05889) | |
| 2022 | Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2208.13720) | |
| 2022 | SNIFF: Reverse Engineering of Neural Networks With Fault Attacks | Neural Networks | IEEE Transactions on Reliability | [Link](https://ieeexplore.ieee.org/document/9530205) | |
| 2022 | High-Fidelity Model Extraction Attacks via Remote Power Monitors | Machine Learning Models | AICAS | [Link](https://ieeexplore.ieee.org/document/9869973) | |

### 2021

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2021 | GraphMI: Extracting Private Graph Data from Graph Neural Networks | Graph Neural Networks | IJCAI | [Link](https://www.ijcai.org/proceedings/2021/516) | |
| 2021 | Extracting Training Data from Large Language Models | Large Language Models | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) | |
| 2021 | MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2021/html/Kariyappa_MAZE_Data-Free_Model_Stealing_Attack_Using_Zeroth-Order_Gradient_Estimation_CVPR_2021_paper.html) | |
| 2021 | Data-Free Model Extraction | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2021/html/Truong_Data-Free_Model_Extraction_CVPR_2021_paper.html) | |
| 2021 | Model Extraction and Adversarial Transferability, Your BERT is Vulnerable! | BERT | arXiv | [Link](http://arxiv.org/abs/2103.10013) | |
| 2021 | Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2010.12751) | |
| 2021 | Watermarking Graph Neural Networks by Random Graphs | Graph Neural Networks | ISDFS | [Link](http://arxiv.org/abs/2011.00512) | |
| 2021 | Model Stealing Attacks Against Inductive Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2112.08331) | |
| 2021 | Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture | Neural Networks | GLSVLSI | [Link](https://doi.org/10.1145/3453688.3461512) | |
| 2021 | Towards Extracting Graph Neural Network Models via Prediction Queries (Student Abstract) | Graph Neural Networks | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/17959) | |
| 2021 | GraphMI: Extracting Private Graph Data from Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2106.02820) | |
| 2021 | Stealing Machine Learning Parameters via Side Channel Power Attacks | Machine Learning Models | ISVLSI | [Link](https://ieeexplore.ieee.org/abstract/document/9516772) | |
| 2021 | Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks | GANs | ACSAC | [Link](https://doi.org/10.1145/3485832.3485838) | |
| 2021 | Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2104.05921) | |
| 2021 | SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries | Machine Learning Models | AISec | [Link](https://doi.org/10.1145/3474369.3486863) | |
| 2021 | Stealing Deep Reinforcement Learning Models for Fun and Profit | Deep Reinforcement Learning Models | ASIA CCS | [Link](https://doi.org/10.1145/3433210.3453090) | |
| 2021 | InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion | Machine Learning Models | IJCAI | [Link](https://www.ijcai.org/proceedings/2021/336) | |
| 2021 | Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction | Sequential Recommenders | RecSys | [Link](https://doi.org/10.1145/3460231.3474275) | |
| 2021 | Hermes Attack: Steal DNN Models with Lossless Inference Accuracy | Deep Neural Networks | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity21/presentation/zhu) | |
| 2021 | Deep Neural Network Fingerprinting by Conferrable Adversarial Examples | Deep Neural Networks | arXiv | [Link](http://arxiv.org/abs/1912.00888) | |
| 2021 | MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI | Explainable AI Models | arXiv | [Link](http://arxiv.org/abs/2107.08909) | |
| 2021 | Model Extraction and Adversarial Transferability, Your BERT is Vulnerable! | BERT | NAACL | [Link](https://aclanthology.org/2021.naacl-main.161) | |
| 2021 | Leveraging Partial Model Extractions using Uncertainty Quantification | Machine Learning Models | CloudNet | [Link](https://ieeexplore.ieee.org/document/9657130) | |
| 2021 | Model Extraction and Adversarial Attacks on Neural Networks Using Switching Power Information | Neural Networks | ICANN | | |

### 2020

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2020 | DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints | Deep Neural Networks | ASPLOS | [Link](https://doi.org/10.1145/3373376.3378460) | |
| 2020 | Practical Side-Channel Based Model Extraction Attack on Tree-Based Machine Learning Algorithm | Tree-Based ML Models | ACNS Workshops | | |
| 2020 | Stealing Links from Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2005.02131) | |
| 2020 | Special-Purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries | Machine Learning Models | TrustCom | [Link](https://ieeexplore.ieee.org/abstract/document/9343086) | |
| 2020 | Extraction of Complex DNN Models: Real Threat or Boogeyman? | Deep Neural Networks | EDSMLS | | |
| 2020 | ActiveThief: Model Extraction Using Active Learning and Unannotated Public Data | Machine Learning Models | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/5432) | |
| 2020 | Cryptanalytic Extraction of Neural Network Models | Neural Networks | CRYPTO | | |
| 2020 | Stealing Your Data from Compressed Machine Learning Models | Machine Learning Models | DAC | [Link](https://ieeexplore.ieee.org/abstract/document/9218633) | |
| 2020 | Model Extraction Attacks on Recurrent Neural Networks | Recurrent Neural Networks | Journal of Information Processing | | |
| 2020 | Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints | Neural Networks | ASPLOS | [Link](http://arxiv.org/abs/1903.03916) | |
| 2020 | Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel | Deep Neural Networks | DSN | [Link](https://ieeexplore.ieee.org/abstract/document/9153424) | |
| 2020 | Best-Effort Adversarial Approximation of Black-Box Malware Classifiers | Malware Classifiers | arXiv | [Link](http://arxiv.org/abs/2006.15725) | |
| 2020 | High Accuracy and High Fidelity Extraction of Neural Networks | Neural Networks | arXiv | [Link](http://arxiv.org/abs/1909.01838) | |
| 2020 | Thieves on Sesame Street! Model Extraction of BERT-based APIs | BERT | ICLR | [Link](https://iclr.cc/virtual_2020/poster_Byl5NREFDr.html) | |
| 2020 | Exploring connections between active learning and model extraction | Machine Learning Models | USENIX Security | | |
| 2020 | Black-Box Ripper: Copying black-box models using generative evolutionary algorithms | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2010.11158) | |
| 2020 | Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures | Deep Neural Networks | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity20/presentation/yan) | |
| 2020 | Open DNN Box by Power Side-Channel Attack | Deep Neural Networks | IEEE TCAS II | [Link](https://ieeexplore.ieee.org/document/9000972) | |
| 2020 | Reverse-engineering deep ReLU networks | Deep ReLU Networks | ICML | [Link](https://proceedings.mlr.press/v119/rolnick20a.html) | |
| 2020 | Model extraction from counterfactual explanations | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2009.01884) | |
| 2020 | GANRED: GAN-based Reverse Engineering of DNNs via Cache Side-Channel | Deep Neural Networks | CCSW | [Link](https://doi.org/10.1145/3411495.3421356) | |
| 2020 | DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage | Deep Neural Networks | HOST | [Link](https://ieeexplore.ieee.org/document/9300274) | |

### 2019

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2019 | Stealing Neural Networks via Timing Side Channels | Neural Networks | arXiv | [Link](http://arxiv.org/abs/1812.11720) | |
| 2019 | PRADA: Protecting Against DNN Model Stealing Attacks | Deep Neural Networks | EuroS&P | [Link](https://ieeexplore.ieee.org/abstract/document/8806737) | |
| 2019 | A framework for the extraction of Deep Neural Networks by leveraging public data | Deep Neural Networks | arXiv | [Link](http://arxiv.org/abs/1905.09165) | |
| 2019 | Adversarial Model Extraction on Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/1912.07721) | |
| 2019 | Efficiently Stealing your Machine Learning Models | Machine Learning Models | WPES | [Link](https://doi.org/10.1145/3338498.3358646) | |
| 2019 | GDALR: An Efficient Model Duplication Attack on Black Box Machine Learning Models | Machine Learning Models | ICSCAN | [Link](https://ieeexplore.ieee.org/document/8878726) | |
| 2019 | Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data | Deep Neural Networks | IJCNN | [Link](http://arxiv.org/abs/1912.03959) | |
| 2019 | Knockoff Nets: Stealing Functionality of Black-Box Models | Machine Learning Models | CVPR | [Link](https://ieeexplore.ieee.org/document/8953839) | |
| 2019 | Model Reconstruction from Model Explanations | Machine Learning Models | FAT* | [Link](https://doi.org/10.1145/3287560.3287562) | |
| 2019 | Model Weight Theft With Just Noise Inputs: The Curious Case of the Petulant Attacker | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/1912.08987) | |
| 2019 | Model-Extraction Attack Against FPGA-DNN Accelerator Utilizing Correlation Electromagnetic Analysis | DNN Accelerators | FCCM | [Link](https://ieeexplore.ieee.org/document/8735505) | |
| 2019 | CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel | Neural Networks | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity19/presentation/batina) | |

### 2018

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2018 | Model Extraction Warning in MLaaS Paradigm | Machine Learning Models | ACSAC | [Link](https://doi.org/10.1145/3274694.3274740) | |
| 2018 | Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data | CNNs | IJCNN | [Link](http://arxiv.org/abs/1806.05476) | |
| 2018 | Active Deep Learning Attacks under Strict Rate Limitations for Online API Calls | Machine Learning Models | HST | [Link](https://ieeexplore.ieee.org/document/8574124) | |
| 2018 | Stealing Hyperparameters in Machine Learning | Machine Learning Models | IEEE S&P | [Link](https://ieeexplore.ieee.org/document/8418595) | |
| 2018 | Generative Adversarial Networks for Black-Box API Attacks with Limited Training Data | Machine Learning APIs | ISSPIT | [Link](https://ieeexplore.ieee.org/document/8642683) | |
| 2018 | Towards Reverse-Engineering Black-Box Neural Networks | Neural Networks | arXiv | [Link](http://arxiv.org/abs/1711.01768) | |
| 2018 | Reverse engineering convolutional neural networks through side-channel information leaks | CNNs | DAC | [Link](https://doi.org/10.1145/3195970.3196105) | |

### 2017

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2017 | Practical Black-Box Attacks against Machine Learning | Machine Learning Models | ASIA CCS | [Link](https://dl.acm.org/doi/10.1145/3052973.3053009) | |
| 2017 | Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models | Machine Learning Models | arXiv | [Link](https://www.semanticscholar.org/paper/Decision-Based-Adversarial-Attacks%3A-Reliable-Models-Brendel-Rauber/1b225474e7a5794f98cdfbde8b12ccbc56799409) | |
| 2017 | How to steal a machine learning classifier with deep learning | Machine Learning Classifiers | HST | [Link](https://ieeexplore.ieee.org/document/7943475) | |

### 2016

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2016 | Stealing Machine Learning Models via Prediction APIs | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/1609.02943) | |

### 2014

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2014 | Adding Robustness to Support Vector Machines Against Adversarial Reverse Engineering | Support Vector Machines | CIKM | [Link](https://doi.org/10.1145/2661829.2662047) | |

## Membership Inference Defense
### Defense Papers 2023 [[Back to Top](#membership-inference-attacks-and-defenses-on-machine-learning-models-literature)]
| Year   | Title |  Adversarial Knowledge | Target Model  |   Venue  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|---------------|
| 2023 | **Mitigating Membership Inference Attacks via Weighted Smoothing** | Black-box | Classification Models | ACSAC | [Link](https://dl.acm.org/doi/abs/10.1145/3627106.3627189) | [Link](https://github.com/BennyTMT/weighted-smoothing) |
| 2023 | **MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2311.00919) | |
| 2023 | **Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction** | Black-box | Classification Models | NDSS | [Link](https://arxiv.org/abs/2307.01610) | [Link](https://github.com/DependableSystemsLab/MIA_defense_HAMP) |
| 2023 | **LoDen: Making Every Client in Federated Learning a Defender Against the Poisoning Membership Inference Attacks** | White-box; Black-box | Classification Models | Asia CCS | [Link](https://dl.acm.org/doi/abs/10.1145/3579856.3590334) | [Link](https://github.com/UQ-Trust-Lab/LoDen) |

### Defense Papers 2022 [[Back to Top](#membership-inference-attacks-and-defenses-on-machine-learning-models-literature)]
| Year   | Title |  Adversarial Knowledge | Target Model  |   Venue  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|---------------|
| 2022 | **Defense against membership inference attack in graph neural networks through graph perturbation** | White-box | Graph Embedding Models | Int. J. Inf. Secur. | [Link](https://link.springer.com/article/10.1007/s10207-022-00646-y) | |
| 2022 | **Provable Membership Inference Privacy** | White-box; Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2211.06582) | |
| 2022 | **Repeated Knowledge Distillation with Confidence Masking to Mitigate Membership Inference Attacks** | White-box; Black-box | Classification Models | AISec | [Link](https://dl.acm.org/doi/10.1145/3560830.3563721) | |
| 2022 | **NeuGuard: Lightweight Neuron-Guided Defense against Membership Inference Attacks** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2206.05565) | |
| 2022 | **Defending against Membership Inference Attacks with High Utility by GAN** | White-box; Black-box | Classification Models | TDSC | [Link](https://ieeexplore.ieee.org/abstract/document/9773984) | | 
| 2022 | **RelaxLoss: Defending Membership Inference Attacks without Losing Utility** | White-box; Black-box | Classification Models | ICLR | [Link](https://openreview.net/forum?id=FEDfGWVZYIn) | [Link](https://github.com/DingfanChen/RelaxLoss) |
| 2022 | **Assessing Differentially Private Variational Autoencoders under Membership Inference** | Black-box | Generative Models | Arxiv | [Link](https://arxiv.org/abs/2204.07877) | [Link](https://github.com/SAP-samples/security-research-vae-dp-mia) |
| 2022 | **Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation** | Black-box | Image Translation Models | Arxiv | [Link](https://arxiv.org/abs/2203.05212) | |
| 2022 | **MIAShield: Defending Membership Inference Attacks via Preemptive Exclusion of Members** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2203.00915) | |
| 2022 | **Privacy-preserving Generative Framework Against Membership Inference Attacks** | White-box; Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2202.05469) | |

### Defense Papers 2021 [[Back to Top](#membership-inference-attacks-and-defenses-on-machine-learning-models-literature)]
| Year   | Title |  Adversarial Knowledge | Target Model  |   Venue  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|---------------|
| 2021 | **Enhanced Mixup Training: a Defense Method Against Membership Inference Attack** | Black-box | Classification Models | ISPEC | [Link](https://link.springer.com/chapter/10.1007/978-3-030-93206-0_3) | |
| 2021 | **Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture** | White-box; Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2110.08324) | |
| 2021 | **On the privacy-utility trade-off in differentially private hierarchical text classification** | White-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2103.02895) | |
| 2021 | **MLCapsule: Guarded Offline Deployment of Machine Learning as a Service** | Black-box | Classification Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Hanzlik_MLCapsule_Guarded_Offline_Deployment_of_Machine_Learning_as_a_Service_CVPRW_2021_paper.html) | |
| 2021 | **Comparing Local and Central Differential Privacy Using Membership Inference Attacks** | White-box | Classification Models | DBSec | [Link](https://link.springer.com/chapter/10.1007/978-3-030-81242-3_2) | [Link](https://github.com/SAP-samples/security-research-membership-inference-and-differential-privacy)|
| 2021 | **Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning** | White-box | Classification Models | S&P | [Link](https://conferences.computer.org/sp/pdfs/sp/2021/893400a866.pdf) | |
| 2021 | **When Does Data Augmentation Help With Membership Inference Attacks?** | Black-box | Classification Models | ICML | [Link](http://proceedings.mlr.press/v139/kaya21a.html) | [Link](https://github.com/yigitcankaya/augmentation_mia) |
| 2021 | **Against Membership Inference Attack: Pruning is All You Need** | Black-box | Classification Models | IJCAI | [Link](https://www.ijcai.org/proceedings/2021/0432.pdf) | |
| 2021 | **Membership Privacy for Machine Learning Models Through Knowledge Transfer** | White-box; Black-box | Classification Models | AAAI | [Link](https://www.aaai.org/AAAI21Papers/AAAI-8428.ShejwalkarV.pdf) | |
| 2021 | **Quantifying Membership Privacy via Information Leakage** | Black-box | Classification Models | IEEE Trans. Inf. Forensics Secur. | [Link](https://ieeexplore.ieee.org/abstract/document/9406982) | |
| 2021 | **Membership Inference Attacks and Defenses in Classification Models** | Black-box | Classification Models | CODASPY | [Link](https://dl.acm.org/doi/abs/10.1145/3422337.3447836?casa_token=OgVGtSGexk4AAAAA:OL1PF_yI4hvGMZRuAg4AmHdGsy8kNbvNDMDCK4Bf-42sGR4PJ0YYdBKwMKaAmUbplZecpyivNS0OmA) | |
| 2021 | **Digestive Neural Networks: A Novel Defense Strategy Against Inference Attacks in Federated Learning** | White-box | Classification Models |  Computers & Security | [Link](https://www.sciencedirect.com/science/article/pii/S0167404821002029) | |
| 2021 | **Resisting Membership Inference Attacks through Knowledge Distillation** | Black-box | Classification Models | Neurocomputing | [Link](https://www.sciencedirect.com/science/article/pii/S0925231221006329?casa_token=c1IzeuYRtpUAAAAA:0wsZqqvYC_MDDxxmJiX6ukGXUz1ZWeXrrs3ZM7HNS22peJvUcI-ED4PuN8RzYTJyMzsTIH-dIvY) | |
| 2021 | **privGAN: Protecting GANs from membership inference attacks at low cost to utility** | White-box | Generative Models | PoPETs| [Link](https://petsymposium.org/2021/files/papers/issue3/popets-2021-0041.pdf) | |
| 2021 | **Generating Private Data Surrogates for Vision Related Tasks** | White-box | Generative Models | ICPR | [Link](https://ieeexplore.ieee.org/abstract/document/9413067?casa_token=-TsmURst7cIAAAAA:7JmyRDPMEqGQumYXCRMMDnqsfw_bQrIbl8Om7GvjU07Py58rR7exbhl6kX3ChllJaLp5hrMXXg) | |
| 2021 | **Membership Inference Attack with Multi-Grade Service Models in Edge Intelligence** | Black-box | Classification Models | IEEE Network | [Link](https://ieeexplore.ieee.org/abstract/document/9355044?casa_token=kRgvvXyazeEAAAAA:op3d_NR3gAdao2j_6pThCiCJYF8YzuAY3wYYPgKfsTRYNZajUjHiF76hiaeBcaqynjDg723Wgg) | |
| 2021 | **PAR-GAN: Improving the Generalization of Generative Adversarial Networks Against Membership Inference Attacks** | White-box | Generative Models | KDD | [Link](https://dl.acm.org/doi/abs/10.1145/3447548.3467445?casa_token=nrRQY7saHGMAAAAA:Hm5z6sD94Titemm-rXcuZ9SWCvxlmuYxYIzMzw9szKC5zo2ZKf5X_RJ0mK5qMfetLCdPG6LzaAnOTQ) | [Link](https://github.com/shilab/PAR-GAN)|
| 2021 | **Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods: Application to Retinal Diagnostics** | Black-box | Classification Models | MICCAI Workshop | [Link](https://arxiv.org/abs/2103.03078) | |
| 2021 | **Defending Privacy Against More Knowledgeable Membership Inference Attackers** | White-box; Black-box | Classification Models | KDD | [Link](https://dl.acm.org/doi/abs/10.1145/3447548.3467444) | [Link](https://github.com/yyinfaramita/Crystal) |
   
### Defense Papers 2020 [[Back to Top](#membership-inference-attacks-and-defenses-on-machine-learning-models-literature)]
| Year   | Title |  Adversarial Knowledge | Target Model  |   Venue  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|---------------|
| 2020 | **Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2001.08855) | |
| 2020 | **Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack**  | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2001.08855) | |
| 2020 | **Differential Privacy Protection Against Membership Inference Attack on Machine Learning for Genomic Data** | Black-box | Classification Models | Biocomputing | [Link](https://www.worldscientific.com/doi/abs/10.1142/9789811232701_0003) | |
| 2020 | **A Secure Federated Learning Framework for 5G Networks** | White-box | Classification Models | IEEE Wireless Communications | [Link](https://ieeexplore.ieee.org/abstract/document/9170265?casa_token=mQO49_zNRZ8AAAAA:WY1Jk5fA6olK16zTOpOkE8pHUkGWMA7wvStZIAUwYTHzZ0hSeFUW_-xY2SwNl7usgf4xFQY7OQ) | |
| 2020 | **Auditing Differentially Private Machine Learning: How Private is Private SGD?** | Black-box | Classification Models | NeurIPS | [Link](https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html) | [Link](https://github.com/jagielski/auditing-dpsgd) |
| 2020 | **Toward Robustness and Privacy in Federated Learning: Experimenting with Local and Central Differential Privacy** | White-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2009.03561) | |
| 2020 | **Defending Model Inversion and Membership Inference Attacks via Prediction Purification** | Black-box | Classification | Arxiv | [Link](https://arxiv.org/abs/2005.03915) | | 
| 2020 | **Alleviating Privacy Attacks via Causal Learning** | Black-box | Classification Models | ICML | [Link](http://proceedings.mlr.press/v119/tople20a.html) | [Link](https://github.com/microsoft/robustdg) |
| 2020 | **On the Effectiveness of Regularization Against Membership Inference Attacks** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2006.05336) | |
| 2020 | **Characterizing Membership Privacy in Stochastic Gradient Langevin Dynamics** | Black-box | Classification Models | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/6107) | |
| 2020 | **Differentially Private Learning Does Not Bound Membership Inference** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/2010.12112) | |
| 2020 | **Privacy-Preserving in Defending against Membership Inference Attacks** | Black-box | Classification Models | PPMLP | [Link](https://dl.acm.org/doi/abs/10.1145/3411501.3419428?casa_token=ndUU_v6d6HwAAAAA:-dFHWyvi34EVE97Dl8J-k-hoTwqTu6Z8I6Lz5IBYpMss_ogfP0OeP8-fLRxQRhfdy6B0AluwZ_XlPw) | |


### Defense Papers 2019 [[Back to Top](#membership-inference-attacks-and-defenses-on-machine-learning-models-literature)]
| Year   | Title |  Adversarial Knowledge | Target Model  |   Venue  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|---------------|
| 2019 | **Evaluating Differentially Private Machine Learning in Practice** | Black-box | Classification Models | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity19/presentation/jayaraman) | [Link](https://github.com/bargavj/EvaluatingDPML) |
| 2019 | **MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples** | Black-box | Classification Models | CCS | [Link](https://dl.acm.org/doi/abs/10.1145/3319535.3363201) | [Link](https://github.com/jjy1994/MemGuard) |
| 2019 | **Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection** | White-box; Black-box | Generative Models | NeurIPS | [Link](https://papers.nips.cc/paper/2019/hash/47d1e990583c9c67424d369f3414728e-Abstract.html) | |
| 2019 | **Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/1912.11279) | |
| 2019 | **ML Defense: Against Prediction API Threats in Cloud-Based Machine Learning Service** | Black-box | Classification Models | IWQoS | [Link](https://dl.acm.org/doi/abs/10.1145/3326285.3329042) |  |
| 2019 | **Effects of Differential Privacy and Data Skewness on Membership Inference Vulnerability** | Black-box | Classification Models | TPS-ISA | [Link](https://ieeexplore.ieee.org/document/9014384) | | 
| 2019 | **Generating Artificial Data for Private Deep Learning** | Black-box | Generative Models | PAL | [Link](https://infoscience.epfl.ch/record/269025) | |



### Defense Papers 2018 [[Back to Top](#membership-inference-attacks-and-defenses-on-machine-learning-models-literature)]
| Year   | Title |  Adversarial Knowledge | Target Model  |   Venue  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|---------------|
| 2018 | **Machine Learning with Membership Privacy using Adversarial Regularization** | Black-box | Classification Models | CCS | [Link](https://dl.acm.org/doi/abs/10.1145/3243734.3243855) | [Link](https://github.com/SPIN-UMass/ML-Privacy-Regulization) |
| 2018 | **Privacy-preserving Machine Learning through Data Obfuscation** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/1807.01860) | |
| 2018 | **Differentially Private Data Generative Models** | Black-box | Classification Models | Arxiv | [Link](https://arxiv.org/abs/1812.02274) | |
| 2018 | **Membership Inference Attack against Differentially Private Deep Learning Model** | Black-box | Classification Models | Transactions on Data Privacy | [Link](http://www.tdp.cat/issues16/tdp.a289a17.pdf) | |



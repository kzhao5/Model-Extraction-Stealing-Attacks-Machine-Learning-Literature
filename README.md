# Model Extraction(Stealing) Attacks and Defenses on Machine Learning Models Literature

This repository contains a curated list of research papers on model extraction attacks and defenses in machine learning, organized by year of publication.

Papers are sorted by their released dates in descending order.

## Survey Papers
| Year | Title | Venue | Paper Link | Code Link |
|------|-------|-------|------------|-----------|
| 2024 | A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges | arXiv | [Link](http://arxiv.org/abs/2403.04468) | |
| 2024 | Graph neural networks: a survey on the links between privacy and security | Artificial Intelligence Review | [Link](https://doi.org/10.1007/s10462-023-10656-4) | |
| 2024 | Trustworthy Graph Neural Networks: Aspects, Methods and Trends | Proceedings of the IEEE | [Link](http://arxiv.org/abs/2205.07424) | |
| 2024 | Safety in Graph Machine Learning: Threats and Safeguards | arXiv | [Link](http://arxiv.org/abs/2405.11034) | |
| 2024 | SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice | USENIX | [Link](https://www.usenix.org/conference/usenixsecurity24/presentation/nayan) | |
| 2024 | Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey | arXiv | [Link](https://arxiv.org/abs/2406.07973) | |
| 2023 | A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications | arXiv | [Link](http://arxiv.org/abs/2308.16375) | |
| 2023 | A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability | arXiv | [Link](http://arxiv.org/abs/2204.08570) | |
| 2023 | A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly | ScienceDirect | [Link](https://www.sciencedirect.com/science/article/pii/S266729522400014X) | |
| 2023 | I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences | ACM Computing Surveys | [Link](https://doi.org/10.1145/3595292) | |
| 2023 | A Taxonomic Survey of Model Extraction Attacks | IEEE International Conference on Cyber Security and Resilience (CSR) | [Link](https://ieeexplore.ieee.org/document/10224959) | |
| 2023 | A Survey of Privacy Attacks in Machine Learning | ACM Computing Surveys | [Link](https://dl.acm.org/doi/full/10.1145/3624010) | |
| 2022 | Privacy and Robustness in Federated Learning: Attacks and Defenses | IEEE Transactions on Neural Networks and Learning Systems | [Link](https://ieeexplore.ieee.org/abstract/document/9945997?casa_token=x9pGEwKp2ocAAAAA:hln9lI37WYKjlaMyIM6J1CaBM_UzTUw3EeIARefaHEbzXXSEPyQt0tPFuREPlvtTBIiUpoVYQj0) | |
| 2022 | Towards Security Threats of Deep Learning Systems: A Survey | IEEE Transactions on Software Engineering | [Link](https://ieeexplore.ieee.org/document/9252914) | |
| 2021 | A Critical Overview of Privacy in Machine Learning | IEEE Security & Privacy | [Link](https://ieeexplore.ieee.org/abstract/document/9433648?casa_token=3sYP7jF9ZSsAAAAA:AsdWPPO6SrWSz_6WgXoB-EIaaqbRSvZBdPyWICAXqmykYteyt-DRplHFXvmedAOQpfboF60T) | |

## Model Extraction Attack

### Table of Contents

- [2024](#2024)
- [2023](#2023)
- [2022](#2022)
- [2021](#2021)
- [2020](#2020)
- [2019](#2019)
- [2018](#2018)
- [2017](#2017)
- [2016](#2016)
- [2014](#2014)

### 2024

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2024 | Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2405.05990) | |
| 2024 | LLM-FIN: Large Language Models Fingerprinting Attack on Edge Devices | Large Language Models | ISQED | [Link](https://ieeexplore.ieee.org/abstract/document/10528736) | |
| 2024 | Model Extraction Attack against On-device Deep Learning with Power Side Channel | Deep Neural Networks | ISQED | [Link](https://ieeexplore.ieee.org/abstract/document/10528716) | |
| 2024 | Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2405.12295) | |
| 2024 | A realistic model extraction attack against graph neural networks | Graph Neural Networks | Knowledge-Based Systems | [Link](https://www.sciencedirect.com/science/article/pii/S0950705124007780) | |
| 2024 | Large Language Models for Link Stealing Attacks Against Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2406.16963) | |
| 2024 | Link Stealing Attacks Against Inductive Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2405.05784) | |
| 2024 | Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels | CNN | arXiv | [Link](http://arxiv.org/abs/2402.11953) | |
| 2024 | SwiftTheft: A Time-Efficient Model Extraction Attack Framework Against Cloud-Based Deep Neural Networks | Deep Neural Networks | Chinese Journal of Electronics | [Link](https://ieeexplore.ieee.org/abstract/document/10410588) | |
| 2024 | Model Extraction Attack Without Natural Images | Deep Neural Networks | ACNS Workshops | | |
| 2024 | DEMISTIFY: Identifying On-device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps | Mobile ML Models | ICSE | [Link](https://doi.org/10.1145/3597503.3623325) | |
| 2024 | Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble Based Sample Selection | Deep Neural Networks | WACV | [Link](https://openaccess.thecvf.com/content/WACV2024/html/Jindal_Army_of_Thieves_Enhancing_Black-Box_Model_Extraction_via_Ensemble_Based_WACV_2024_paper.html) | |
| 2024 | Trained to Leak: Hiding Trojan Side-Channels in Neural Network Weights | Neural Networks | HOST | [Link](https://ieeexplore.ieee.org/abstract/document/10545350) | |
| 2024 | A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning | Split Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_A_Stealthy_Wrongdoer_Feature-Oriented_Reconstruction_Attack_against_Split_Learning_CVPR_2024_paper.html) | |
| 2024 | Prompt Stealing Attacks Against Large Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2402.12959) | |
| 2024 | Stealing Part of a Production Language Model | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2403.06634) | |
| 2024 | Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing | Deep Neural Networks | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Fully_Exploiting_Every_Real_Sample_SuperPixel_Sample_Gradient_Model_Stealing_CVPR_2024_paper.html) | |
| 2024 | Teach LLMs to Phish: Stealing Private Information from Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2403.00871) | |
| 2024 | Data-Free Hard-Label Robustness Stealing Attack | Deep Neural Networks | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/28510) | |
| 2024 | Large Language Model Watermark Stealing With Mixed Integer Programming | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2405.19677) | |
| 2024 | PRSA: PRompt Stealing Attacks against Large Language Models | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2402.19200) | |
| 2024 | QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines | Quantum Neural Networks | arXiv | [Link](http://arxiv.org/abs/2403.10790) | |
| 2024 | Privacy Backdoors: Stealing Data with Corrupted Pretrained Models | Pretrained Models | arXiv | [Link](http://arxiv.org/abs/2404.00473) | |
| 2024 | AugSteal: Advancing Model Steal With Data Augmentation in Active Learning Frameworks | Active Learning Models | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10490222) | |
| 2024 | Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation | GANs | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/29966) | |
| 2024 | Stealing Image-to-Image Translation Models With a Single Query | Image-to-Image Translation Models | arXiv | [Link](http://arxiv.org/abs/2406.00828) | |
| 2024 | A two-stage model extraction attack on GANs with a small collected dataset | GANs | Computers & Security | [Link](https://www.sciencedirect.com/science/article/pii/S0167404823005448) | |
| 2024 | Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks | Deep Neural Networks | IEEE TCAD | [Link](https://ieeexplore.ieee.org/abstract/document/10500842) | |
| 2024 | COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2402.08679) | |
| 2024 | Prompt Stealing Attacks Against Text-to-Image Generation Models | Text-to-Image Models | arXiv | [Link](http://arxiv.org/abs/2302.09923) | |
| 2024 | An Empirical Evaluation of the Data Leakage in Federated Graph Learning | GNNs | IEEE Transactions on Network Science and Engineering | [Link](https://ieeexplore.ieee.org/document/10288405) | |
| 2024 | AttacKG+:Boosting Attack Knowledge Graph Construction with Large Language Models | LLMs | arXiv | [Link](http://arxiv.org/abs/2405.04753) | |

### 2023

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2023 | Explanation leaks: Explanation-guided model extraction attacks | Explainable AI Models | Information Sciences | [Link](https://www.sciencedirect.com/science/article/pii/S002002552300316X) | |
| 2023 | Model Leeching: An Extraction Attack Targeting LLMs | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2309.10544) | |
| 2023 | Are Diffusion Models Vulnerable to Membership Inference Attacks? | Diffusion Models | ICML | [Link](https://proceedings.mlr.press/v202/duan23b.html) | |
| 2023 | Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2307.04401) | |
| 2023 | Model Extraction Attacks on Split Federated Learning | Federated Learning Models | arXiv | [Link](http://arxiv.org/abs/2303.08581) | |
| 2023 | Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders | Image Encoders | arXiv | [Link](http://arxiv.org/abs/2201.07513) | |
| 2023 | Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2312.10943) | |
| 2023 | Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack | Machine Learning Models | NeurIPS | [Link](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e5440ffceaf4831b5f98652b8a27ffde-Abstract-Conference.html) | |
| 2023 | D-DAE: Defense-Penetrating Model Extraction Attacks | Deep Neural Networks | IEEE S&P | [Link](https://ieeexplore.ieee.org/abstract/document/10179406) | |
| 2023 | Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models | Image Translation Models | arXiv | [Link](http://arxiv.org/abs/2104.12623) | |
| 2023 | AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models | White-Box Models | arXiv | [Link](http://arxiv.org/abs/2302.02162) | |
| 2023 | Efficient Model Extraction by Data Set Stealing, Balancing, and Filtering | Machine Learning Models | IEEE IoT Journal | [Link](https://ieeexplore.ieee.org/abstract/document/10214537) | |
| 2023 | DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer | Machine Learning Models | IEEE TDSC | [Link](https://ieeexplore.ieee.org/abstract/document/10007048) | |
| 2023 | Model Leeching: An Extraction Attack Targeting LLMs | LLMs | arXiv | [Link](https://arxiv.org/abs/2309.10544) | |
### 2022

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2022 | Canary Extraction in Natural Language Understanding Models | NLU Models | arXiv | [Link](http://arxiv.org/abs/2203.13920) | |
| 2022 | Are Large Pre-Trained Language Models Leaking Your Personal Information? | Large Language Models | arXiv | [Link](http://arxiv.org/abs/2205.12628) | |
| 2022 | Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers | Transformers | arXiv | [Link](http://arxiv.org/abs/2209.10505) | |
| 2022 | Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices | Deep Learning Models | ESORICS | | |
| 2022 | DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories | Deep Neural Networks | IEEE S&P | [Link](https://ieeexplore.ieee.org/abstract/document/9833743) | |
| 2022 | DualCF: Efficient Model Extraction Attack from Counterfactual Explanations | Machine Learning Models | FAccT | [Link](https://doi.org/10.1145/3531146.3533188) | |
| 2022 | GAME: Generative-Based Adaptive Model Extraction Attack | Machine Learning Models | ESORICS | | |
| 2022 | Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks | Deep Neural Networks | NeurIPS | [Link](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ed189de2611f200bd4c2ab30c576e99e-Abstract-Conference.html) | |
| 2022 | On the Difficulty of Defending Self-Supervised Learning against Model Extraction | Self-Supervised Learning Models | ICML | [Link](https://proceedings.mlr.press/v162/dziedzic22a.html) | |
| 2022 | Black-Box Dissector: Towards Erasing-Based Hard-Label Model Stealing Attack | Machine Learning Models | ECCV | | |
| 2022 | Imitated Detectors: Stealing Knowledge of Black-box Object Detectors | Object Detectors | ACM MM | [Link](https://doi.org/10.1145/3503161.3548416) | |
| 2022 | StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning | Self-Supervised Learning Models | ACM CCS | [Link](https://doi.org/10.1145/3548606.3560586) | |
| 2022 | Model Stealing Attacks Against Vision-Language Models | Vision-Language Models | OpenReview | [Link](https://openreview.net/forum?id=v-rx235RlfI) | |
| 2022 | ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles | Deep Neural Networks | IEEE TETCI | [Link](https://ieeexplore.ieee.org/abstract/document/9726514) | |
| 2022 | Data Stealing Attack on Medical Images: Is It Safe to Export Networks from Data Lakes? | Medical Image Models | MICCAI | | |
| 2022 | Towards Data-Free Model Stealing in a Hard Label Setting | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2022/html/Sanyal_Towards_Data-Free_Model_Stealing_in_a_Hard_Label_Setting_CVPR_2022_paper.html) | |
| 2022 | Careful What You Wish For: on the Extraction of Adversarially Trained Models | Adversarially Trained Models | PST | [Link](https://ieeexplore.ieee.org/abstract/document/9851981) | |
| 2022 | Enhance Model Stealing Attack via Label Refining | Machine Learning Models | ICSP | [Link](https://ieeexplore.ieee.org/document/9778562) | |
| 2022 | StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning | Self-Supervised Learning Models | arXiv | [Link](http://arxiv.org/abs/2201.05889) | |
| 2022 | Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2208.13720) | |
| 2022 | SNIFF: Reverse Engineering of Neural Networks With Fault Attacks | Neural Networks | IEEE Transactions on Reliability | [Link](https://ieeexplore.ieee.org/document/9530205) | |
| 2022 | High-Fidelity Model Extraction Attacks via Remote Power Monitors | Machine Learning Models | AICAS | [Link](https://ieeexplore.ieee.org/document/9869973) | |

### 2021

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2021 | GraphMI: Extracting Private Graph Data from Graph Neural Networks | Graph Neural Networks | IJCAI | [Link](https://www.ijcai.org/proceedings/2021/516) | |
| 2021 | Extracting Training Data from Large Language Models | Large Language Models | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) | |
| 2021 | MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2021/html/Kariyappa_MAZE_Data-Free_Model_Stealing_Attack_Using_Zeroth-Order_Gradient_Estimation_CVPR_2021_paper.html) | |
| 2021 | Data-Free Model Extraction | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2021/html/Truong_Data-Free_Model_Extraction_CVPR_2021_paper.html) | |
| 2021 | Model Extraction and Adversarial Transferability, Your BERT is Vulnerable! | BERT | arXiv | [Link](http://arxiv.org/abs/2103.10013) | |
| 2021 | Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2010.12751) | |
| 2021 | Watermarking Graph Neural Networks by Random Graphs | Graph Neural Networks | ISDFS | [Link](http://arxiv.org/abs/2011.00512) | |
| 2021 | Model Stealing Attacks Against Inductive Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2112.08331) | |
| 2021 | Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture | Neural Networks | GLSVLSI | [Link](https://doi.org/10.1145/3453688.3461512) | |
| 2021 | Towards Extracting Graph Neural Network Models via Prediction Queries (Student Abstract) | Graph Neural Networks | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/17959) | |
| 2021 | GraphMI: Extracting Private Graph Data from Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2106.02820) | |
| 2021 | Stealing Machine Learning Parameters via Side Channel Power Attacks | Machine Learning Models | ISVLSI | [Link](https://ieeexplore.ieee.org/abstract/document/9516772) | |
| 2021 | Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks | GANs | ACSAC | [Link](https://doi.org/10.1145/3485832.3485838) | |
| 2021 | Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2104.05921) | |
| 2021 | SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries | Machine Learning Models | AISec | [Link](https://doi.org/10.1145/3474369.3486863) | |
| 2021 | Stealing Deep Reinforcement Learning Models for Fun and Profit | Deep Reinforcement Learning Models | ASIA CCS | [Link](https://doi.org/10.1145/3433210.3453090) | |
| 2021 | InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion | Machine Learning Models | IJCAI | [Link](https://www.ijcai.org/proceedings/2021/336) | |
| 2021 | Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction | Sequential Recommenders | RecSys | [Link](https://doi.org/10.1145/3460231.3474275) | |
| 2021 | Hermes Attack: Steal DNN Models with Lossless Inference Accuracy | Deep Neural Networks | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity21/presentation/zhu) | |
| 2021 | Deep Neural Network Fingerprinting by Conferrable Adversarial Examples | Deep Neural Networks | arXiv | [Link](http://arxiv.org/abs/1912.00888) | |
| 2021 | MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI | Explainable AI Models | arXiv | [Link](http://arxiv.org/abs/2107.08909) | |
| 2021 | Model Extraction and Adversarial Transferability, Your BERT is Vulnerable! | BERT | NAACL | [Link](https://aclanthology.org/2021.naacl-main.161) | |
| 2021 | Leveraging Partial Model Extractions using Uncertainty Quantification | Machine Learning Models | CloudNet | [Link](https://ieeexplore.ieee.org/document/9657130) | |
| 2021 | Model Extraction and Adversarial Attacks on Neural Networks Using Switching Power Information | Neural Networks | ICANN | | |

### 2020

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2020 | DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints | Deep Neural Networks | ASPLOS | [Link](https://doi.org/10.1145/3373376.3378460) | |
| 2020 | Practical Side-Channel Based Model Extraction Attack on Tree-Based Machine Learning Algorithm | Tree-Based ML Models | ACNS Workshops | | |
| 2020 | Stealing Links from Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2005.02131) | |
| 2020 | Special-Purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries | Machine Learning Models | TrustCom | [Link](https://ieeexplore.ieee.org/abstract/document/9343086) | |
| 2020 | Extraction of Complex DNN Models: Real Threat or Boogeyman? | Deep Neural Networks | EDSMLS | | |
| 2020 | ActiveThief: Model Extraction Using Active Learning and Unannotated Public Data | Machine Learning Models | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/5432) | |
| 2020 | Cryptanalytic Extraction of Neural Network Models | Neural Networks | CRYPTO | | |
| 2020 | Stealing Your Data from Compressed Machine Learning Models | Machine Learning Models | DAC | [Link](https://ieeexplore.ieee.org/abstract/document/9218633) | |
| 2020 | Model Extraction Attacks on Recurrent Neural Networks | Recurrent Neural Networks | Journal of Information Processing | | |
| 2020 | Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints | Neural Networks | ASPLOS | [Link](http://arxiv.org/abs/1903.03916) | |
| 2020 | Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel | Deep Neural Networks | DSN | [Link](https://ieeexplore.ieee.org/abstract/document/9153424) | |
| 2020 | Best-Effort Adversarial Approximation of Black-Box Malware Classifiers | Malware Classifiers | arXiv | [Link](http://arxiv.org/abs/2006.15725) | |
| 2020 | High Accuracy and High Fidelity Extraction of Neural Networks | Neural Networks | arXiv | [Link](http://arxiv.org/abs/1909.01838) | |
| 2020 | Thieves on Sesame Street! Model Extraction of BERT-based APIs | BERT | ICLR | [Link](https://iclr.cc/virtual_2020/poster_Byl5NREFDr.html) | |
| 2020 | Exploring connections between active learning and model extraction | Machine Learning Models | USENIX Security | | |
| 2020 | Black-Box Ripper: Copying black-box models using generative evolutionary algorithms | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2010.11158) | |
| 2020 | Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures | Deep Neural Networks | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity20/presentation/yan) | |
| 2020 | Open DNN Box by Power Side-Channel Attack | Deep Neural Networks | IEEE TCAS II | [Link](https://ieeexplore.ieee.org/document/9000972) | |
| 2020 | Reverse-engineering deep ReLU networks | Deep ReLU Networks | ICML | [Link](https://proceedings.mlr.press/v119/rolnick20a.html) | |
| 2020 | Model extraction from counterfactual explanations | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2009.01884) | |
| 2020 | GANRED: GAN-based Reverse Engineering of DNNs via Cache Side-Channel | Deep Neural Networks | CCSW | [Link](https://doi.org/10.1145/3411495.3421356) | |
| 2020 | DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage | Deep Neural Networks | HOST | [Link](https://ieeexplore.ieee.org/document/9300274) | |

### 2019

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2019 | Stealing Neural Networks via Timing Side Channels | Neural Networks | arXiv | [Link](http://arxiv.org/abs/1812.11720) | |
| 2019 | PRADA: Protecting Against DNN Model Stealing Attacks | Deep Neural Networks | EuroS&P | [Link](https://ieeexplore.ieee.org/abstract/document/8806737) | |
| 2019 | A framework for the extraction of Deep Neural Networks by leveraging public data | Deep Neural Networks | arXiv | [Link](http://arxiv.org/abs/1905.09165) | |
| 2019 | Adversarial Model Extraction on Graph Neural Networks | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/1912.07721) | |
| 2019 | Efficiently Stealing your Machine Learning Models | Machine Learning Models | WPES | [Link](https://doi.org/10.1145/3338498.3358646) | |
| 2019 | GDALR: An Efficient Model Duplication Attack on Black Box Machine Learning Models | Machine Learning Models | ICSCAN | [Link](https://ieeexplore.ieee.org/document/8878726) | |
| 2019 | Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data | Deep Neural Networks | IJCNN | [Link](http://arxiv.org/abs/1912.03959) | |
| 2019 | Knockoff Nets: Stealing Functionality of Black-Box Models | Machine Learning Models | CVPR | [Link](https://ieeexplore.ieee.org/document/8953839) | |
| 2019 | Model Reconstruction from Model Explanations | Machine Learning Models | FAT* | [Link](https://doi.org/10.1145/3287560.3287562) | |
| 2019 | Model Weight Theft With Just Noise Inputs: The Curious Case of the Petulant Attacker | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/1912.08987) | |
| 2019 | Model-Extraction Attack Against FPGA-DNN Accelerator Utilizing Correlation Electromagnetic Analysis | DNN Accelerators | FCCM | [Link](https://ieeexplore.ieee.org/document/8735505) | |
| 2019 | CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel | Neural Networks | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity19/presentation/batina) | |

### 2018

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2018 | Model Extraction Warning in MLaaS Paradigm | Machine Learning Models | ACSAC | [Link](https://doi.org/10.1145/3274694.3274740) | |
| 2018 | Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data | CNNs | IJCNN | [Link](http://arxiv.org/abs/1806.05476) | |
| 2018 | Active Deep Learning Attacks under Strict Rate Limitations for Online API Calls | Machine Learning Models | HST | [Link](https://ieeexplore.ieee.org/document/8574124) | |
| 2018 | Stealing Hyperparameters in Machine Learning | Machine Learning Models | IEEE S&P | [Link](https://ieeexplore.ieee.org/document/8418595) | |
| 2018 | Generative Adversarial Networks for Black-Box API Attacks with Limited Training Data | Machine Learning APIs | ISSPIT | [Link](https://ieeexplore.ieee.org/document/8642683) | |
| 2018 | Towards Reverse-Engineering Black-Box Neural Networks | Neural Networks | arXiv | [Link](http://arxiv.org/abs/1711.01768) | |
| 2018 | Reverse engineering convolutional neural networks through side-channel information leaks | CNNs | DAC | [Link](https://doi.org/10.1145/3195970.3196105) | |

### 2017

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2017 | Practical Black-Box Attacks against Machine Learning | Machine Learning Models | ASIA CCS | [Link](https://dl.acm.org/doi/10.1145/3052973.3053009) | |
| 2017 | Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models | Machine Learning Models | arXiv | [Link](https://www.semanticscholar.org/paper/Decision-Based-Adversarial-Attacks%3A-Reliable-Models-Brendel-Rauber/1b225474e7a5794f98cdfbde8b12ccbc56799409) | |
| 2017 | How to steal a machine learning classifier with deep learning | Machine Learning Classifiers | HST | [Link](https://ieeexplore.ieee.org/document/7943475) | |

### 2016

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2016 | Stealing Machine Learning Models via Prediction APIs | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/1609.02943) | |

### 2014

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2014 | Adding Robustness to Support Vector Machines Against Adversarial Reverse Engineering | Support Vector Machines | CIKM | [Link](https://doi.org/10.1145/2661829.2662047) | |

## Model Extraction Defense

### Table of Contents

- [2024](#2024)
- [2023](#2023)
- [2022](#2022)
- [2021](#2021)
- [2020](#2020)
- [2019](#2019)
- [2018](#2018)

### 2024

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2024 | Defense Against Model Extraction Attacks on Recommender Systems | Recommender Systems | WSDM | [Link](https://dl.acm.org/doi/10.1145/3616855.3635751) | |
| 2024 | GNNGuard: A Fingerprinting Framework for Verifying Ownerships of Graph Neural Networks | Graph Neural Networks | OpenReview | [Link](https://openreview.net/forum?id=RNl51vzvDE) | |
| 2024 | Privacy-Enhanced Graph Neural Network for Decentralized Local Graphs | Graph Neural Networks | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10305592) | |
| 2024 | GENIE: Watermarking Graph Neural Networks for Link Prediction | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2406.04805) | |
| 2024 | PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2402.04435) | |
| 2024 | Inversion-Guided Defense: Detecting Model Stealing Attacks by Output Inverting | Machine Learning Models | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10466709) | |
| 2024 | Defending against model extraction attacks with OOD feature learning and decision boundary confusion | Machine Learning Models | Computers & Security | [Link](https://www.sciencedirect.com/science/article/pii/S016740482300473X) | |
| 2024 | SAME: Sample Reconstruction against Model Extraction Attacks | Machine Learning Models | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/29974) | |
| 2024 | Model Stealing Detection for IoT Services Based on Multi-Dimensional Features | IoT Models | IEEE IoT Journal | [Link](https://ieeexplore.ieee.org/abstract/document/10510375) | |
| 2024 | MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction | Deep Learning Models | arXiv | [Link](http://arxiv.org/abs/2403.18580) | |
| 2024 | Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2403.05181) | |
| 2024 | Construct a Secure CNN Against Gradient Inversion Attack | CNN | PAKDD | | |
| 2024 | A Comprehensive Defense Framework Against Model Extraction Attacks | Machine Learning Models | IEEE TDSC | [Link](https://ieeexplore.ieee.org/abstract/document/10080996) | |
| 2024 | HODA: Hardness-Oriented Detection of Model Extraction Attacks | Machine Learning Models | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10266376) | |
| 2024 | Adaptive and robust watermark against model extraction attack | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2405.02365) | |
| 2024 | Defense against Model Extraction Attack by Bayesian Active Watermarking | Machine Learning Models | OpenReview | [Link](https://openreview.net/forum?id=EFtNP211X3) | |
| 2024 | Poisoning-Free Defense Against Black-Box Model Extraction | Machine Learning Models | ICASSP | [Link](https://ieeexplore.ieee.org/abstract/document/10447550) | |
| 2024 | Reliable Model Watermarking: Defending Against Theft without Compromising on Evasion | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2404.13518) | |
| 2024 | Efficient Model Stealing Defense with Noise Transition Matrix | Machine Learning Models | CVPR | [Link](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Efficient_Model_Stealing_Defense_with_Noise_Transition_Matrix_CVPR_2024_paper.html) | |
| 2024 | Making models more secure: An efficient model stealing detection method | Machine Learning Models | Computers and Electrical Engineering | [Link](https://www.sciencedirect.com/science/article/pii/S0045790624001940) | |
| 2024 | TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment | Transformer Models | arXiv | [Link](http://arxiv.org/abs/2404.11121) | |
| 2024 | Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data | Deep Neural Networks | arXiv | [Link](http://arxiv.org/abs/2403.10663) | |
| 2024 | QUEEN: Query Unlearning against Model Extraction | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2407.01251) | |
| 2024 | Bident Structure for Neural Network Model Protection | Neural Networks | SCITEPRESS | [Link](https://www.scitepress.org/Link.aspx?doi=10.5220/0008923403770384) | |

### 2023

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2023 | GrOVe: Ownership Verification of Graph Neural Networks using Embeddings | Graph Neural Networks | arXiv | [Link](http://arxiv.org/abs/2304.08566) | |
| 2023 | Making Watermark Survive Model Extraction Attacks in Graph Neural Networks | Graph Neural Networks | | | |
| 2023 | Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks | Machine Learning Models | CIKM | [Link](https://doi.org/10.1145/3583780.3615211) | |
| 2023 | Defending against model extraction attacks with physical unclonable function | Machine Learning Models | Information Sciences | [Link](https://www.sciencedirect.com/science/article/pii/S0020025523001147) | |
| 2023 | Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks | Deep Neural Networks | ACM MM | [Link](https://doi.org/10.1145/3581783.3612092) | |
| 2023 | APMSA: Adversarial Perturbation Against Model Stealing Attacks | Machine Learning Models | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10049136) | |
| 2023 | Deep Neural Network Watermarking against Model Extraction Attack | Deep Neural Networks | ACM MM | [Link](https://doi.org/10.1145/3581783.3612515) | |
| 2023 | Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders | Encoder Models | NeurIPS | [Link](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad1efab57a04d93f097e7fbb2d4fc054-Abstract-Conference.html) | |
| 2023 | Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times | Deep Neural Networks | IEEE TIFS | [Link](https://ieeexplore.ieee.org/abstract/document/10042038) | |

### 2022

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2022 | CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks | Text Generation Models | arXiv | [Link](http://arxiv.org/abs/2209.08773) | |
| 2022 | Defending against Model Stealing via Verifying Embedded External Features | Machine Learning Models | AAAI | [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20036) | |
| 2022 | Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack | Machine Learning Models | IEEE TDSC | [Link](https://ieeexplore.ieee.org/abstract/document/9389670) | |
| 2022 | DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking | Deep Learning Models | arXiv | [Link](http://arxiv.org/abs/2207.13321) | |
| 2022 | SeInspect: Defending Model Stealing via Heterogeneous Semantic Inspection | Machine Learning Models | ESORICS | | |
| 2022 | Model Stealing Defense against Exploiting Information Leak through the Interpretation of Deep Neural Nets | Deep Neural Networks | IJCAI | [Link](https://www.ijcai.org/proceedings/2022/100) | |
| 2022 | How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2206.14157) | |

### 2021

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2021 | Watermarking Graph Neural Networks by Random Graphs | Graph Neural Networks | ISDFS | [Link](http://arxiv.org/abs/2011.00512) | |
| 2021 | BODAME: Bilevel Optimization for Defense Against Model Extraction | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2103.06797) | |
| 2021 | A protection method of trained CNN model with a secret key from unauthorized access | CNN | APSIPA TSIP | [Link](https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/protection-method-of-trained-cnn-model-with-a-secret-key-from-unauthorized-access/9D3C17F7C8F42945A6D1577C86ACFFF8) | |
| 2021 | SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries | Machine Learning Models | AISec | [Link](https://doi.org/10.1145/3474369.3486863) | |
| 2021 | Entangled Watermarks as a Defense against Model Extraction | Machine Learning Models | USENIX Security | [Link](https://www.usenix.org/conference/usenixsecurity21/presentation/jia) | |
| 2021 | Stateful Detection of Model Extraction Attacks | Machine Learning Models | arXiv | [Link](http://arxiv.org/abs/2107.05166) | |
| 2021 | NeurObfuscator: A Full-stack Obfuscation Tool to Mitigate Neural Architecture Stealing | Neural Networks | HOST | [Link](https://ieeexplore.ieee.org/abstract/document/9702279) | |
| 2021 | DAS-AST: Defending Against Model Stealing Attacks Based on Adaptive Softmax Transformation | Machine Learning Models | ISC | | |
| 2021 | DAWN: Dynamic Adversarial Watermarking of Neural Networks | Neural Networks | ACM MM | [Link](https://doi.org/10.1145/3474085.3475591) | |

### 2020

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2020 | Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks | Deep Neural Networks | arXiv | [Link](http://arxiv.org/abs/1906.10908) | |
| 2020 | Perturbing Inputs to Prevent Model Stealing | Machine Learning Models | CNS | [Link](https://ieeexplore.ieee.org/document/9162336) | |
| 2020 | Defending Against Model Stealing Attacks With Adaptive Misinformation | Machine Learning Models | CVPR | [Link](https://ieeexplore.ieee.org/document/9157021) | |
| 2020 | Protecting DNNs from Theft using an Ensemble of Diverse Models | Deep Neural Networks | OpenReview | [Link](https://openreview.net/forum?id=LucJxySuJcE) | |
| 2020 | A Protection against the Extraction of Neural Network Models | Neural Networks | arXiv | [Link](http://arxiv.org/abs/2005.12782) | |
| 2020 | Preventing Neural Network Weight Stealing via Network Obfuscation | Neural Networks | Intelligent Computing | | |

### 2019

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2019 | Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations | Neural Networks | SPW | [Link](https://ieeexplore.ieee.org/document/8844598) | |
| 2019 | PRADA: Protecting Against DNN Model Stealing Attacks | Deep Neural Networks | EuroS&P | [Link](https://ieeexplore.ieee.org/abstract/document/8806737) | |
| 2019 | BDPL: A Boundary Differentially Private Layer Against Machine Learning Model Extraction Attacks | Machine Learning Models | ESORICS | | |

### 2018

| Year | Title | Target Model | Venue | Paper Link | Code Link |
|------|-------|--------------|-------|------------|-----------|
| 2018 | Model Extraction Warning in MLaaS Paradigm | Machine Learning Models | ACSAC | [Link](https://doi.org/10.1145/3274694.3274740) | |
